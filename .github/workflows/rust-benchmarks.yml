name: Rust Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run benchmarks daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      baseline:
        description: 'Baseline commit to compare against'
        required: false
        type: string
      quick_mode:
        description: 'Run quick benchmarks only'
        required: false
        default: false
        type: boolean

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  rust-benchmark:
    name: Rust Performance Benchmarks on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy
    
    - name: Cache dependencies
      uses: Swatinem/rust-cache@v2
      with:
        key: benchmark-${{ matrix.os }}
    
    - name: Install system dependencies (Ubuntu)
      if: matrix.os == 'ubuntu-latest'
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential
    
    - name: Build project in release mode
      run: cargo build --release --workspace
    
    - name: Run integration tests first
      run: cargo test --test integration_tests --release
    
    - name: Run performance benchmarks
      run: |
        mkdir -p benchmark_results
        if [ "${{ inputs.quick_mode }}" = "true" ]; then
          cargo bench --bench performance_benchmarks -- --measurement-time 10 --sample-size 50
        else
          cargo bench --bench performance_benchmarks
        fi
    
    - name: Generate benchmark report
      shell: bash
      run: |
        # Create a summary of benchmark results
        echo "# Performance Benchmark Results" > benchmark_results/summary.md
        echo "Generated: $(date)" >> benchmark_results/summary.md
        echo "OS: ${{ matrix.os }}" >> benchmark_results/summary.md
        echo "Rust Version: $(rustc --version)" >> benchmark_results/summary.md
        echo "" >> benchmark_results/summary.md
        
        # Add system information
        echo "## System Information" >> benchmark_results/summary.md
        if [ "${{ matrix.os }}" = "ubuntu-latest" ]; then
          echo "CPU: $(lscpu | grep 'Model name' | cut -d: -f2 | xargs)" >> benchmark_results/summary.md
          echo "Memory: $(free -h | grep Mem | awk '{print $2}')" >> benchmark_results/summary.md
        elif [ "${{ matrix.os }}" = "macos-latest" ]; then
          echo "CPU: $(sysctl -n machdep.cpu.brand_string)" >> benchmark_results/summary.md
          echo "Memory: $(sysctl -n hw.memsize | awk '{print $1/1024/1024/1024 " GB"}')" >> benchmark_results/summary.md
        fi
        echo "" >> benchmark_results/summary.md
        
        # Copy criterion results
        if [ -d "target/criterion" ]; then
          cp -r target/criterion benchmark_results/
        fi
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: rust-benchmark-results-${{ matrix.os }}
        path: |
          benchmark_results/
          target/criterion/
        retention-days: 30
    
    - name: Compare with baseline (if provided)
      if: inputs.baseline != ''
      shell: bash
      run: |
        echo "Comparing with baseline: ${{ inputs.baseline }}"
        # This would require implementing baseline comparison logic
        # For now, just log the baseline commit
        git log --oneline -1 ${{ inputs.baseline }} || echo "Baseline commit not found"

  performance-regression-check:
    name: Performance Regression Check
    runs-on: ubuntu-latest
    needs: rust-benchmark
    if: github.event_name == 'pull_request'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download benchmark results
      uses: actions/download-artifact@v4
      with:
        name: rust-benchmark-results-ubuntu-latest
        path: current_results
    
    - name: Check for performance regressions
      run: |
        echo "# Performance Regression Analysis" > regression_report.md
        echo "This is a placeholder for performance regression detection." >> regression_report.md
        echo "In a full implementation, this would:" >> regression_report.md
        echo "1. Compare current results with main branch baseline" >> regression_report.md
        echo "2. Identify significant performance changes (>5% regression)" >> regression_report.md
        echo "3. Generate alerts for critical regressions" >> regression_report.md
        echo "4. Provide recommendations for optimization" >> regression_report.md
        
        # For now, just check if benchmark files exist
        if [ -d "current_results/criterion" ]; then
          echo "✅ Benchmark results found and appear valid" >> regression_report.md
        else
          echo "❌ Benchmark results missing or invalid" >> regression_report.md
          exit 1
        fi
    
    - name: Comment on PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('regression_report.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `## 📊 Rust Performance Benchmark Results\n\n${report}`
          });

  benchmark-trend-analysis:
    name: Benchmark Trend Analysis
    runs-on: ubuntu-latest
    needs: rust-benchmark
    if: github.event_name == 'schedule' || github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download benchmark results
      uses: actions/download-artifact@v4
      with:
        name: rust-benchmark-results-ubuntu-latest
        path: latest_results
    
    - name: Store results for trend analysis
      run: |
        # In a real implementation, this would store results in a database
        # or append to a historical results file for trend analysis
        echo "Storing benchmark results for trend analysis..."
        echo "Commit: ${{ github.sha }}" > latest_results/metadata.txt
        echo "Date: $(date -u)" >> latest_results/metadata.txt
        echo "Branch: ${{ github.ref_name }}" >> latest_results/metadata.txt
    
    - name: Generate trend report
      run: |
        echo "# Performance Trend Analysis" > trend_report.md
        echo "Generated: $(date)" >> trend_report.md
        echo "" >> trend_report.md
        echo "This would contain:" >> trend_report.md
        echo "- Performance trends over time" >> trend_report.md
        echo "- Identification of performance improvements/regressions" >> trend_report.md
        echo "- Recommendations for optimization" >> trend_report.md
        echo "- Hardware-specific performance characteristics" >> trend_report.md
    
    - name: Upload trend analysis
      uses: actions/upload-artifact@v4
      with:
        name: trend-analysis
        path: trend_report.md
        retention-days: 90
