name: PR Performance Check

on:
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: false
        default: 'quick'
        type: choice
        options:
          - quick
          - comparison

jobs:
  benchmark:
    runs-on: ubuntu-latest
    # Only run quick benchmarks for PR performance checks
    if: github.event_name == 'pull_request' || inputs.benchmark_type == 'quick' || inputs.benchmark_type == 'comparison'

    steps:
    - uses: actions/checkout@v4
      with:
        submodules: recursive

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install uv
      uses: astral-sh/setup-uv@v5
      with:
        enable-cache: true
        cache-dependency-glob: "pyproject.toml"

    # Install build tools for Linux
    - name: Install build tools
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          build-essential \
          binutils \
          pkg-config \
          libssl-dev \
          lld \
          clang

    - name: Set up Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy

    # Set stable build environment for Linux
    - name: Set build environment
      run: |
        echo "CARGO_NET_GIT_FETCH_WITH_CLI=true" >> $GITHUB_ENV
        echo "RUSTFLAGS=-C opt-level=3" >> $GITHUB_ENV
        echo "CC=clang" >> $GITHUB_ENV
        echo "CXX=clang++" >> $GITHUB_ENV
        echo "RUSTFLAGS=-C opt-level=3 -C linker=clang" >> $GITHUB_ENV
        echo "âœ… Using clang as compiler and linker"

    - name: Install system tools
      shell: bash
      run: |
        case "${{ runner.os }}" in
          Linux)
            # Install hyperfine on Ubuntu
            wget https://github.com/sharkdp/hyperfine/releases/download/v1.18.0/hyperfine_1.18.0_amd64.deb
            sudo dpkg -i hyperfine_1.18.0_amd64.deb
            ;;
          macOS)
            # Install hyperfine on macOS
            brew install hyperfine
            ;;
          Windows)
            # Install hyperfine on Windows
            choco install hyperfine
            ;;
        esac

        # Verify installation
        hyperfine --version

    - name: Cache Rust dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-

    - name: Install dependencies
      run: |
        uv sync --group testing

    - name: Verify pytest-benchmark installation
      run: |
        uv run python -c "import pytest_benchmark; print('pytest-benchmark installed successfully')"
        uv run pytest --version
        uv run pytest --help | grep -E "(benchmark|codspeed)" || echo "Benchmark plugins not found"

    - name: Build project
      run: |
        uv run nox -s build

    - name: Verify installation
      run: |
        uv run python -c "import ferrocp; print(f'ferrocp imported successfully')"

    - name: Generate test data
      run: |
        uv run python benchmarks/data/generate_test_data.py --output-dir benchmarks/data/test_files

    - name: Run quick benchmarks
      if: github.event_name == 'pull_request' || inputs.benchmark_type == 'quick'
      run: |
        mkdir -p benchmarks/results
        uv run pytest benchmarks/ --benchmark-only --benchmark-sort=mean -k "small_file or medium_file" --benchmark-json=benchmarks/results/benchmark-quick-ubuntu-py3.11.json

    - name: Run comparison benchmarks
      if: inputs.benchmark_type == 'comparison'
      run: |
        mkdir -p benchmarks/results
        uv run pytest benchmarks/test_comparison.py --benchmark-only --benchmark-sort=mean --benchmark-json=benchmarks/results/benchmark-compare-ubuntu-py3.11.json

    - name: Run Rust benchmarks
      if: inputs.benchmark_type == 'rust' || inputs.benchmark_type == 'all'
      run: |
        mkdir -p benchmarks/results/rust
        # Run integration tests first to ensure everything works
        cargo test --test integration_tests --release

        # Run performance benchmarks with appropriate settings
        if [ "${{ inputs.benchmark_type }}" = "quick" ]; then
          cargo bench --bench performance_benchmarks -- --measurement-time 10 --sample-size 50
        else
          cargo bench --bench performance_benchmarks
        fi

        # Also run the copy benchmarks
        cargo bench --bench copy_benchmarks

    - name: Generate benchmark summary
      run: |
        # Create a summary of benchmark results
        mkdir -p benchmarks/results
        echo "# PR Performance Check Results" > benchmarks/results/summary.md
        echo "Generated: $(date)" >> benchmarks/results/summary.md
        echo "OS: ubuntu-latest" >> benchmarks/results/summary.md
        echo "Python Version: 3.11" >> benchmarks/results/summary.md
        echo "Rust Version: $(rustc --version)" >> benchmarks/results/summary.md
        echo "" >> benchmarks/results/summary.md

        # Add system information
        echo "## System Information" >> benchmarks/results/summary.md
        echo "CPU: $(lscpu | grep 'Model name' | cut -d: -f2 | xargs)" >> benchmarks/results/summary.md
        echo "Memory: $(free -h | grep Mem | awk '{print $2}')" >> benchmarks/results/summary.md
        echo "" >> benchmarks/results/summary.md

        # Copy criterion results if they exist
        if [ -d "target/criterion" ]; then
          cp -r target/criterion benchmarks/results/
          echo "## Criterion Results" >> benchmarks/results/summary.md
          echo "Detailed benchmark results are available in the criterion directory." >> benchmarks/results/summary.md
        fi

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: pr-benchmark-results
        path: |
          benchmarks/results/*.json
          benchmarks/results/summary.md
          target/criterion/
        retention-days: 30

    - name: Comment PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const comment = `
          ## ðŸš€ PR Performance Check Results

          Quick performance benchmarks have been completed for this PR.

          **ðŸ“Š Artifacts Available:**
          - [Benchmark Results](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})
          - [Summary Report](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})

          *Note: Full performance analysis is now handled by GoReleaser during releases.*
          `;

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
